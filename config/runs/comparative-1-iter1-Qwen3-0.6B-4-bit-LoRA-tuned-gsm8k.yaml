run_id: comparative-1-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k
method: baseline
model:
  name: Qwen/Qwen3-0.6B
  precision: 4bit
  adapter:
    type: lora
    r: 64
    alpha: 16
    dropout: 0.05
dataset:
  name: gsm8k
  config: main
  max_seq_length: 512
  text_column: question
  label_column: answer
  pack_sequences: true
training:
  epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 8
  optimizer:
    type: adamw
    learning_rate: 2e-5
    weight_decay: 0.1
    betas: [0.9, 0.999]
    eps: 1e-8
  scheduler:
    type: cosine
    warmup_steps: 500
  layer_lr_scaling:
    enabled: false
    beta: 1.0
    strategy: none
evaluation:
  decoding_temperature: 0.0
  strategy: greedy
hardware:
  device: cpu
  max_memory_mb: 500
optuna:
  n_trials: 20
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-5
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.1
