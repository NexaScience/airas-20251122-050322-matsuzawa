{
  "research_topic": "Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems",
  "queries": [
    "Qwen3-0.6B fine-tuning",
    "learning rate optimization",
    "GSM8K fine-tuning",
    "adaptive learning rate",
    "elementary math LLM"
  ],
  "research_study_list": [
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
      "meta_data": {
        "arxiv_id": "2305.14314v1",
        "authors": [
          "Tim Dettmers",
          "Artidoro Pagnoni",
          "Ari Holtzman",
          "Luke Zettlemoyer"
        ],
        "published_date": "2023-05-23T17:50:33Z",
        "pdf_url": "https://arxiv.org/pdf/2305.14314v1.pdf"
      }
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
      "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR applied to every $τ$ steps that minimizes the resulted validation loss. We solve this black-box optimization on the fly by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for $τ$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only $τ'\\llτ$ steps and train an exponential model to predict the validation loss after $τ$ steps. This mutual-training process between BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of $1.31\\times$ over state-of-the-art heavily-tuned LR schedules.",
      "meta_data": {
        "arxiv_id": "2105.10762v1",
        "authors": [
          "Yuchen Jin",
          "Tianyi Zhou",
          "Liangyu Zhao",
          "Yibo Zhu",
          "Chuanxiong Guo",
          "Marco Canini",
          "Arvind Krishnamurthy"
        ],
        "published_date": "2021-05-22T16:41:10Z",
        "pdf_url": "https://arxiv.org/pdf/2105.10762v1.pdf"
      }
    },
    {
      "title": "Reverse engineering learned optimizers reveals known and novel mechanisms",
      "abstract": "Learned optimizers are algorithms that can themselves be trained to solve optimization problems. In contrast to baseline optimizers (such as momentum or Adam) that use simple update rules derived from theoretical principles, learned optimizers use flexible, high-dimensional, nonlinear parameterizations. Although this can lead to better performance in certain settings, their inner workings remain a mystery. How is a learned optimizer able to outperform a well tuned baseline? Has it learned a sophisticated combination of existing optimization techniques, or is it implementing completely new behavior? In this work, we address these questions by careful analysis and visualization of learned optimizers. We study learned optimizers trained from scratch on three disparate tasks, and discover that they have learned interpretable mechanisms, including: momentum, gradient clipping, learning rate schedules, and a new form of learning rate adaptation. Moreover, we show how the dynamics of learned optimizers enables these behaviors. Our results help elucidate the previously murky understanding of how learned optimizers work, and establish tools for interpreting future learned optimizers.",
      "meta_data": {
        "arxiv_id": "2011.02159v2",
        "authors": [
          "Niru Maheswaranathan",
          "David Sussillo",
          "Luke Metz",
          "Ruoxi Sun",
          "Jascha Sohl-Dickstein"
        ],
        "published_date": "2020-11-04T07:12:43Z",
        "pdf_url": "https://arxiv.org/pdf/2011.02159v2.pdf"
      }
    },
    {
      "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
      "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
      "meta_data": {
        "arxiv_id": "2309.12284v4",
        "authors": [
          "Longhui Yu",
          "Weisen Jiang",
          "Han Shi",
          "Jincheng Yu",
          "Zhengying Liu",
          "Yu Zhang",
          "James T. Kwok",
          "Zhenguo Li",
          "Adrian Weller",
          "Weiyang Liu"
        ],
        "published_date": "2023-09-21T17:45:42Z",
        "pdf_url": "https://arxiv.org/pdf/2309.12284v4.pdf"
      }
    },
    {
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
      "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",
      "meta_data": {
        "arxiv_id": "2402.10176v2",
        "authors": [
          "Shubham Toshniwal",
          "Ivan Moshkov",
          "Sean Narenthiran",
          "Daria Gitman",
          "Fei Jia",
          "Igor Gitman"
        ],
        "published_date": "2024-02-15T18:26:11Z",
        "pdf_url": "https://arxiv.org/pdf/2402.10176v2.pdf"
      }
    },
    {
      "title": "MoMo: Momentum Models for Adaptive Learning Rates",
      "abstract": "Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic gradient descent with momentum). MoMo uses momentum estimates of the losses and gradients sampled at each iteration to build a model of the loss function. Our model makes use of any known lower bound of the loss function by using truncation, e.g. most losses are lower-bounded by zero. The model is then approximately minimized at each iteration to compute the next step. We show how MoMo can be used in combination with any momentum-based method, and showcase this by developing MoMo-Adam, which is Adam with our new model-based adaptive learning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for convex problems with interpolation, needing knowledge of no problem-specific quantities other than the optimal value. Additionally, for losses with unknown lower bounds, we develop on-the-fly estimates of a lower bound, that are incorporated in our model. We show that MoMo and MoMo-Adam improve over SGD-M and Adam in terms of robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR, and Imagenet, for recommender systems on Criteo, for a transformer model on the translation task IWSLT14, and for a diffusion model.",
      "meta_data": {
        "arxiv_id": "2305.07583v3",
        "authors": [
          "Fabian Schaipp",
          "Ruben Ohana",
          "Michael Eickenberg",
          "Aaron Defazio",
          "Robert M. Gower"
        ],
        "published_date": "2023-05-12T16:25:57Z",
        "pdf_url": "https://arxiv.org/pdf/2305.07583v3.pdf"
      }
    },
    {
      "title": "Llemma: An Open Language Model for Mathematics",
      "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
      "meta_data": {
        "arxiv_id": "2310.10631v3",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "published_date": "2023-10-16T17:54:07Z",
        "pdf_url": "https://arxiv.org/pdf/2310.10631v3.pdf"
      }
    },
    {
      "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
      "abstract": "Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with several families of models showing evidence of systematic overfitting across almost all model sizes. Further analysis suggests a positive relationship (Spearman's r^2 = 0.36) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that some models may have partially memorized GSM8k. Nevertheless, many models, especially those on the frontier, show minimal signs of overfitting, and all models broadly demonstrate generalization to novel math problems guaranteed to not be in their training data.",
      "meta_data": {
        "arxiv_id": "2405.00332v4",
        "authors": [
          "Hugh Zhang",
          "Jeff Da",
          "Dean Lee",
          "Vaughn Robinson",
          "Catherine Wu",
          "Will Song",
          "Tiffany Zhao",
          "Pranav Raja",
          "Charlotte Zhuang",
          "Dylan Slack",
          "Qin Lyu",
          "Sean Hendryx",
          "Russell Kaplan",
          "Michele Lunati",
          "Summer Yue"
        ],
        "published_date": "2024-05-01T05:52:05Z",
        "pdf_url": "https://arxiv.org/pdf/2405.00332v4.pdf"
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "Current fine-tuning of large LLMs such as Qwen3-0.6B on GSM8K uses a single global learning-rate schedule. This often leads to (1) excessive updates on the upper transformer blocks, causing catastrophic forgetting, while (2) lower blocks remain under-adapted, limiting mathematical reasoning depth. The key limitation is the lack of layer-wise differentiation in learning-rate magnitude, which could be solved with a very small change to the optimisation procedure.",
        "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
        "experimental_setup": "1. Model: Qwen3-0.6B (HF Transformers)\n2. Dataset: GSM8K train / test split (as in Cobbe et al.)\n3. Baseline: Standard fine-tuning with global learning rate 2e-5, 500 warm-up steps, cosine decay.\n4. Proposed: Same schedule + ALLRS with β=0.9.\n5. Optimiser: AdamW, weight-decay 0.1.\n6. Training budget: 3 epochs, batch size 8 (fits on a single A100 with gradient accumulation).\n7. Evaluation: Generate answers with greedy decoding (temperature=0) and compare against ground-truth numeric answers.\n8. Comparison: Report accuracy on GSM8K test set; measure training stability (loss curve).",
        "primary_metric": "accuracy",
        "experimental_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# build parameter groups with layer-wise lr scale\nbeta = 0.9\nlayer_to_lr_scale = {}\nfor name, _ in model.named_parameters():\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])   # transformer.h.{idx}.*\n        layer_to_lr_scale.setdefault(layer_id, beta**(model.config.num_hidden_layers-1-layer_id))\n\nparam_groups = []\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])\n        scale = layer_to_lr_scale[layer_id]\n    else:\n        scale = 1.0  # embeddings, ln_f, lm_head\n    param_groups.append({\"params\": [param], \"lr_scale\": scale})\n\nglobal_lr = 2e-5\noptimizer = AdamW(param_groups, lr=global_lr, weight_decay=0.1)\n\n# attach scheduler\nnum_training_steps = 6000\nnum_warmup_steps = 500\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\n# in the training loop\nfor step, batch in enumerate(train_loader):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    # apply individual lr scales\n    for g in optimizer.param_groups:\n        g[\"lr\"] = scheduler.get_last_lr()[0] * g[\"lr_scale\"]",
        "expected_result": "Baseline fine-tuning (global LR): ~55% GSM8K accuracy, occasional training instability after epoch 2.\nALLRS (β=0.9): expected 58-60% accuracy (+3-5 points) with smoother validation loss curve and no extra compute cost. Per-token perplexity should drop ~3% relative to baseline by the end of training.",
        "expected_conclusion": "A single multiplicative depth factor (β) is enough to rectify the imbalance of updates across transformer layers. This minimal change is trivial to implement, adds negligible overhead, and empirically improves GSM8K accuracy by several points, demonstrating that careful layer-wise learning-rate allocation is a practical lever for boosting mathematical reasoning in LLM fine-tuning."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces an exponential layer-wise decay factor β during fine-tuning of Qwen3-0.6B. However, layer-wise learning-rate decay (LLRD) or scaling has already been explored in several contexts: (1) BERT fine-tuning for NLP (e.g., Howard & Ruder 2018 ‘ULMFiT’, Sun et al. 2019 ‘Fine-tuning BERT…’), (2) ViT training where an almost identical formula lr_ℓ = lr·β^(L-ℓ) is standard, and (3) LARS/LAMB optimizers that adaptively modify per-layer update magnitude. The only novel aspect is applying this well-known trick specifically to GSM8K mathematical-reasoning fine-tuning of a small Qwen model and empirically framing it as a remedy for catastrophic forgetting in upper blocks. No new algorithmic insight beyond prior LLRD literature is presented; the work is an application/confirmation in a new data-model regime rather than a conceptual advance.",
        "novelty_score": 4,
        "significance_reason": "Despite limited methodological novelty, the hypothesis could have practical impact: GSM8K is a widely-used benchmark and Qwen3-0.6B is within the resource envelope of many practitioners. If a single hyper-parameter can add 3–5 percentage-points of accuracy with zero extra compute, the technique is valuable for the community fine-tuning small LLMs under tight budgets. Academically, it provides evidence that layer-wise LR allocation helps reasoning tasks—not just generic GLUE or vision. Societally, improved math-reasoning at lower compute cost broadens access to educational or tutoring systems. However, the incremental nature of the gain and the narrow scope (one model, one dataset) temper its overall significance.",
        "significance_score": 6
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "Current fine-tuning of large LLMs such as Qwen3-0.6B on GSM8K uses a single global learning-rate schedule. This often leads to (1) excessive updates on the upper transformer blocks, causing catastrophic forgetting, while (2) lower blocks remain under-adapted, limiting mathematical reasoning depth. The key limitation is the lack of layer-wise differentiation in learning-rate magnitude, which could be solved with a very small change to the optimisation procedure.",
      "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
      "experimental_setup": "1. Model: Qwen3-0.6B (HF Transformers)\n2. Dataset: GSM8K train / test split (as in Cobbe et al.)\n3. Baseline: Standard fine-tuning with global learning rate 2e-5, 500 warm-up steps, cosine decay.\n4. Proposed: Same schedule + ALLRS with β=0.9.\n5. Optimiser: AdamW, weight-decay 0.1.\n6. Training budget: 3 epochs, batch size 8 (fits on a single A100 with gradient accumulation).\n7. Evaluation: Generate answers with greedy decoding (temperature=0) and compare against ground-truth numeric answers.\n8. Comparison: Report accuracy on GSM8K test set; measure training stability (loss curve).",
      "primary_metric": "accuracy",
      "experimental_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# build parameter groups with layer-wise lr scale\nbeta = 0.9\nlayer_to_lr_scale = {}\nfor name, _ in model.named_parameters():\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])   # transformer.h.{idx}.*\n        layer_to_lr_scale.setdefault(layer_id, beta**(model.config.num_hidden_layers-1-layer_id))\n\nparam_groups = []\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])\n        scale = layer_to_lr_scale[layer_id]\n    else:\n        scale = 1.0  # embeddings, ln_f, lm_head\n    param_groups.append({\"params\": [param], \"lr_scale\": scale})\n\nglobal_lr = 2e-5\noptimizer = AdamW(param_groups, lr=global_lr, weight_decay=0.1)\n\n# attach scheduler\nnum_training_steps = 6000\nnum_warmup_steps = 500\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\n# in the training loop\nfor step, batch in enumerate(train_loader):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    # apply individual lr scales\n    for g in optimizer.param_groups:\n        g[\"lr\"] = scheduler.get_last_lr()[0] * g[\"lr_scale\"]",
      "expected_result": "Baseline fine-tuning (global LR): ~55% GSM8K accuracy, occasional training instability after epoch 2.\nALLRS (β=0.9): expected 58-60% accuracy (+3-5 points) with smoother validation loss curve and no extra compute cost. Per-token perplexity should drop ~3% relative to baseline by the end of training.",
      "expected_conclusion": "A single multiplicative depth factor (β) is enough to rectify the imbalance of updates across transformer layers. This minimal change is trivial to implement, adds negligible overhead, and empirically improves GSM8K accuracy by several points, demonstrating that careful layer-wise learning-rate allocation is a practical lever for boosting mathematical reasoning in LLM fine-tuning."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
        "experimental_design": {
          "experiment_summary": "Goal: show that Adaptive Layer-wise Learning-Rate Scaling (ALLRS) yields higher answer-accuracy on elementary maths word-problems than the usual single global learning-rate.\nTask: given a GSM8K problem in natural language, generate a step-by-step chain-of-thought that ends with the correct numeric answer. The prediction is considered correct only if the final number exactly matches the gold answer.\nWorkflow:\n1. Load Qwen3-0.6B in 4-bit quantised format to stay inside the 500 MB RAM cap.\n2. Add LoRA adapters to all self-attention and MLP blocks so only ~6 M parameters are trainable.\n3. Prepare the GSM8K train / test splits; tokenise with the Qwen3 tokenizer; pack to 512-token sequences.\n4. Train for 1 epoch with gradient-accumulation to simulate batch-size 8.\n   • Baseline run: AdamW + warm-up 10 % steps + cosine decay, single global learning-rate.\n   • Proposed run: identical schedule but per-block rate lr_l(t)=lr_global(t)·β^(L-l) with β searched in [0.7,0.95].  Implementation: keep one AdamW optimiser, but store a constant lr_scale in every param-group and multiply the scheduler-produced LR by that factor each step.\n5. Decode each test question greedily (temperature 0) and strip everything after the first ‘####’ token to obtain the predicted answer string.\n6. Compute metrics; plot training-loss curves for stability comparison.\n7. Compare final accuracy and learning-curve smoothness; expect +3-5 pp accuracy for ALLRS without extra compute.",
          "evaluation_metrics": [
            {
              "name": "accuracy",
              "description": "Correctness criteria: a prediction is correct if the substring after the first token sequence \"####\" in the model output, stripped of whitespace, exactly equals the gold numeric answer string; for fractional answers the comparison is performed on the unreduced fraction so 3/6≠1/2.\nCalculation: accuracy = (#correct predictions)/(#total test questions).\nTask appropriateness: GSM8K is a single-answer task where each example has one canonical numeric solution; therefore simple accuracy directly measures problem-solving success.\nVisualisations: bar chart comparing baseline vs ALLRS accuracies; 95 % binomial confidence intervals overlaid."
            },
            {
              "name": "training_loss",
              "description": "Correctness criteria: not applicable; this is a descriptive curve.\nCalculation: at every optimisation step record the cross-entropy loss returned by the model; after training, smooth with an exponential moving average (α=0.98) and plot step vs loss.\nTask appropriateness: reveals optimisation stability; a smoother, monotonically decreasing curve indicates better-behaved updates, which ALLRS claims to deliver.\nVisualisations: line plot of smoothed loss for baseline and ALLRS runs on the same axes."
            }
          ],
          "proposed_method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nObjective: mitigate catastrophic forgetting in upper layers and under-adaptation in lower layers during fine-tuning by assigning smaller learning-rates to deeper (closer to embeddings) transformer blocks.\nTheory: let the standard scheduler output a global rate lr_g(t).  For a model with last layer index L, block ℓ uses lr_ℓ(t)=lr_g(t)·β^(L-ℓ), with damping factor 0<β≤1.  Exponential decay maintains the ratio of consecutive layers’ step-sizes, leading to gradient norms that vary smoothly with depth.\nImplementation steps:\n1. Traverse model.named_parameters(); group parameters by transformer block index ℓ, assign constant multiplier β^(L-ℓ) to that group (embeddings, final LN and lm_head get scale 1).\n2. Create a single AdamW optimiser where each param-group stores the field \"lr_scale\".\n3. At every scheduler step, set group[\"lr\"] = scheduler_lr * group[\"lr_scale\"].  No other change to training loop.\n4. Only one new hyper-parameter β is introduced, making grid-search inexpensive.\n5. Compatible with LoRA (only adapter weights receive scaled rates) and with any optimiser that exposes per-group learning-rates.",
          "comparative_methods": [
            "Standard Fine-tuning with single global learning-rate schedule"
          ],
          "models_to_use": [
            "Qwen3-0.6B (4-bit LoRA-tuned)"
          ],
          "datasets_to_use": [
            "gsm8k"
          ],
          "hyperparameters_to_search": [
            {
              "name": "beta",
              "range": "0.7-0.95"
            },
            {
              "name": "learning_rate",
              "range": "1e-5-5e-5"
            },
            {
              "name": "weight_decay",
              "range": "0-0.1"
            }
          ],
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "Qwen/Qwen3-0.6B",
                  "author": "Qwen",
                  "sha": "c1899de289a04d12100db370d81485cdf75e47ca",
                  "created_at": "2025-04-27T03:40:08+00:00",
                  "last_modified": "2025-07-26T03:46:27+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 7136450,
                  "likes": 800,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "LICENSE"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "generation_config.json"
                    },
                    {
                      "rfilename": "merges.txt"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "tokenizer.json"
                    },
                    {
                      "rfilename": "tokenizer_config.json"
                    },
                    {
                      "rfilename": "vocab.json"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "library_name": "transformers",
                    "pipeline_tag": "text-generation",
                    "tags": [],
                    "datasets": [],
                    "base_model": [
                      "Qwen/Qwen3-0.6B-Base"
                    ],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "transformers",
                    "safetensors",
                    "qwen3",
                    "text-generation",
                    "conversational",
                    "arxiv:2505.09388",
                    "base_model:Qwen/Qwen3-0.6B-Base",
                    "base_model:finetune:Qwen/Qwen3-0.6B-Base",
                    "license:apache-2.0",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "deploy:azure",
                    "region:us"
                  ],
                  "pipeline_tag": "text-generation",
                  "library_name": "transformers",
                  "readme": "---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-0.6B-Base\n---\n\n# Qwen3-0.6B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-0.6B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```",
                  "extracted_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-0.6B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)"
                }
              ],
              "datasets": [
                {
                  "id": "openai/gsm8k",
                  "author": "openai",
                  "sha": "e53f048856ff4f594e959d75785d2c2d37b678ee",
                  "created_at": "2022-04-12T10:22:10+00:00",
                  "last_modified": "2024-01-04T12:05:15+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 511811,
                  "likes": 973,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "main/test-00000-of-00001.parquet"
                    },
                    {
                      "rfilename": "main/train-00000-of-00001.parquet"
                    },
                    {
                      "rfilename": "socratic/test-00000-of-00001.parquet"
                    },
                    {
                      "rfilename": "socratic/train-00000-of-00001.parquet"
                    }
                  ],
                  "card_data": {
                    "license": [
                      "mit"
                    ],
                    "language": [
                      "en"
                    ],
                    "tags": [
                      "math-word-problems"
                    ],
                    "datasets": [],
                    "task_categories": [
                      "text2text-generation"
                    ],
                    "size_categories": [
                      "1K<n<10K"
                    ],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "annotations_creators:crowdsourced",
                    "language_creators:crowdsourced",
                    "multilinguality:monolingual",
                    "source_datasets:original",
                    "language:en",
                    "license:mit",
                    "size_categories:10K<n<100K",
                    "format:parquet",
                    "modality:text",
                    "library:datasets",
                    "library:pandas",
                    "library:mlcroissant",
                    "library:polars",
                    "arxiv:2110.14168",
                    "region:us",
                    "math-word-problems"
                  ],
                  "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- crowdsourced\nlanguage:\n- en\nlicense:\n- mit\nmultilinguality:\n- monolingual\nsize_categories:\n- 1K<n<10K\nsource_datasets:\n- original\ntask_categories:\n- text2text-generation\ntask_ids: []\npaperswithcode_id: gsm8k\npretty_name: Grade School Math 8K\ntags:\n- math-word-problems\ndataset_info:\n- config_name: main\n  features:\n  - name: question\n    dtype: string\n  - name: answer\n    dtype: string\n  splits:\n  - name: train\n    num_bytes: 3963202\n    num_examples: 7473\n  - name: test\n    num_bytes: 713732\n    num_examples: 1319\n  download_size: 2725633\n  dataset_size: 4676934\n- config_name: socratic\n  features:\n  - name: question\n    dtype: string\n  - name: answer\n    dtype: string\n  splits:\n  - name: train\n    num_bytes: 5198108\n    num_examples: 7473\n  - name: test\n    num_bytes: 936859\n    num_examples: 1319\n  download_size: 3164254\n  dataset_size: 6134967\nconfigs:\n- config_name: main\n  data_files:\n  - split: train\n    path: main/train-*\n  - split: test\n    path: main/test-*\n- config_name: socratic\n  data_files:\n  - split: train\n    path: socratic/train-*\n  - split: test\n    path: socratic/test-*\n---\n\n# Dataset Card for GSM8K\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-instances)\n  - [Data Splits](#data-instances)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n\n## Dataset Description\n\n- **Homepage:** https://openai.com/blog/grade-school-math/\n- **Repository:** https://github.com/openai/grade-school-math\n- **Paper:** https://arxiv.org/abs/2110.14168\n- **Leaderboard:** [Needs More Information]\n- **Point of Contact:** [Needs More Information]\n\n### Dataset Summary\n\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n- These problems take between 2 and 8 steps to solve.\n- Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer.\n- A bright middle school student should be able to solve every problem: from the paper, \"Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable.\"\n- Solutions are provided in natural language, as opposed to pure math expressions. From the paper: \"We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models’ internal monologues\"\"\n\n### Supported Tasks and Leaderboards\n\nThis dataset is generally used to test logic and math in language modelling.\nIt has been used for many benchmarks, including the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n\n### Languages\n\nThe text in the dataset is in English. The associated BCP-47 code is `en`.\n\n## Dataset Structure\n\n### Data Instances\n\nFor the `main` configuration, each instance contains a string for the grade-school level math question and a string for the corresponding answer with multiple steps of reasoning and calculator annotations (explained [here](https://github.com/openai/grade-school-math#calculation-annotations)).\n\n\n```python\n{\n    'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n    'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n}\n```\n\nFor the `socratic` configuration, each instance contains a string for a grade-school level math question, a string for the corresponding answer with multiple steps of reasoning, calculator annotations (explained [here](https://github.com/openai/grade-school-math#calculation-annotations)), and *Socratic sub-questions*.\n\n```python\n{\n    'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n    'answer': 'How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n}\n```\n\n### Data Fields\n\nThe data fields are the same among `main` and `socratic` configurations and their individual splits.\n\n- question: The question string to a grade school math problem.\n\n- answer: The full solution string to the `question`. It contains multiple steps of reasoning with calculator annotations and the final numeric solution.\n\n### Data Splits\n\n| name   |train|validation|\n|--------|----:|---------:|\n|main    | 7473|      1319|\n|socratic| 7473|      1319|\n\n## Dataset Creation\n\n### Curation Rationale\n\n[Needs More Information]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\nFrom the paper, appendix A:\n\n> We initially collected a starting set of a thousand problems and natural language solutions by hiring freelance contractors on Upwork (upwork.com). We then worked with Surge AI (surgehq.ai), an NLP data labeling platform, to scale up our data collection. After collecting the full dataset, we asked workers to re-solve all problems, with no workers re-solving problems they originally wrote. We checked whether their final answers agreed with the original solutions, and any problems that produced disagreements were either repaired or discarded. We then performed another round of agreement checks on a smaller subset of problems, finding that 1.7% of problems still produce disagreements among contractors. We estimate this to be the fraction of problems that contain breaking errors or ambiguities. It is possible that a larger percentage of problems contain subtle errors.\n\n#### Who are the source language producers?\n\n[Needs More Information]\n\n### Annotations\n\n#### Annotation process\n\n[Needs More Information]\n\n#### Who are the annotators?\n\nSurge AI (surgehq.ai)\n\n### Personal and Sensitive Information\n\n[Needs More Information]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[Needs More Information]\n\n### Discussion of Biases\n\n[Needs More Information]\n\n### Other Known Limitations\n\n[Needs More Information]\n\n## Additional Information\n\n### Dataset Curators\n\n[Needs More Information]\n\n### Licensing Information\n\nThe GSM8K dataset is licensed under the [MIT License](https://opensource.org/licenses/MIT).\n\n### Citation Information\n\n```bibtex\n@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}\n```\n\n### Contributions\n\nThanks to [@jon-tow](https://github.com/jon-tow) for adding this dataset."
                }
              ]
            }
          },
          "experiment_code": {
            "train_py": "import os\nimport json\nimport random\nimport sys\nfrom pathlib import Path\nfrom typing import Tuple, Any, Dict\n\nimport hydra\nimport torch\nfrom omegaconf import DictConfig, OmegaConf, open_dict\nfrom tqdm import tqdm\n\nfrom src.preprocess import build_dataloaders, get_tokenizer\nfrom src.model import (\n    build_optimizer_with_layerwise_scaling,\n    evaluate_accuracy,\n    load_lm_and_prepare_for_training,\n    save_lora_adapters,\n)\n\n# -----------------------------------------------------------------------------\n# Reproducibility helpers\n# -----------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42):\n    \"\"\"Set all relevant random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\n# -----------------------------------------------------------------------------\n# Mode-specific overrides\n# -----------------------------------------------------------------------------\n\ndef _apply_mode_overrides(cfg: DictConfig) -> None:\n    \"\"\"Mutates cfg according to cfg.mode (trial/full).\"\"\"\n    with open_dict(cfg):\n        if cfg.mode == \"trial\":\n            cfg.wandb.mode = \"disabled\"\n            cfg.optuna.n_trials = 0\n            cfg.training.epochs = 1\n            cfg.training.max_train_batches = 2\n        elif cfg.mode == \"full\":\n            cfg.wandb.mode = \"online\"\n        else:\n            raise ValueError(f\"Unknown execution mode '{cfg.mode}'. Allowed: trial | full\")\n\n\n# -----------------------------------------------------------------------------\n# Weights & Biases helpers\n# -----------------------------------------------------------------------------\n\ndef _maybe_init_wandb(cfg: DictConfig):\n    \"\"\"Initialise wandb unless disabled. Returns wandb run or None.\"\"\"\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n\n    import wandb  # local import to avoid mandatory wandb dependency in trial\n\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"[wandb] Run url: {run.get_url()}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Single training pass (no hyper-parameter search)\n# -----------------------------------------------------------------------------\n\ndef _train_once(cfg: DictConfig) -> Tuple[float, float]:\n    \"\"\"Train a model once with fixed hyper-parameters.\n    Returns (best_val_acc, final_test_acc).\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    tokenizer = get_tokenizer(cfg)\n    model = load_lm_and_prepare_for_training(cfg, device)\n\n    train_loader, val_loader, test_loader = build_dataloaders(cfg, tokenizer)\n\n    optimizer, scheduler, _ = build_optimizer_with_layerwise_scaling(cfg, model, len(train_loader))\n\n    wandb_run = _maybe_init_wandb(cfg)\n\n    gradient_accum = cfg.training.gradient_accumulation_steps\n    global_step = 0\n    best_val_acc, best_epoch = 0.0, 0\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        running_loss = 0.0\n        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")\n        for batch_idx, batch in pbar:\n            if cfg.training.max_train_batches and batch_idx >= cfg.training.max_train_batches:\n                break\n\n            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=batch[\"labels\"],\n            )\n            loss = outputs.loss / gradient_accum\n            loss.backward()\n            running_loss += loss.item() * gradient_accum\n\n            if (batch_idx + 1) % gradient_accum == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                global_step += 1\n\n                if wandb_run:\n                    wandb_run.log({\"train_loss\": running_loss / (batch_idx + 1)}, step=global_step)\n\n            pbar.set_postfix({\"loss\": f\"{running_loss / (batch_idx + 1):.4f}\"})\n\n        # ---------------- Validation & Test ----------------\n        val_acc, _, _ = evaluate_accuracy(model, tokenizer, val_loader, device, cfg)\n        if val_acc > best_val_acc:\n            best_val_acc, best_epoch = val_acc, epoch\n\n        test_acc, test_preds, test_golds = evaluate_accuracy(model, tokenizer, test_loader, device, cfg)\n\n        if wandb_run:\n            wandb_run.log({\"val_accuracy\": val_acc, \"test_accuracy\": test_acc}, step=global_step)\n\n        print(f\"Epoch {epoch}: val_acc={val_acc:.4f} | test_acc={test_acc:.4f}\")\n\n    # ---------------- Summary ----------------\n    if wandb_run:\n        wandb_run.summary[\"best_val_accuracy\"] = best_val_acc\n        wandb_run.summary[\"best_epoch\"] = best_epoch\n        wandb_run.summary[\"test_accuracy\"] = test_acc\n        wandb_run.summary[\"_preds\"] = test_preds  # For evaluation script\n        wandb_run.summary[\"_golds\"] = test_golds\n        wandb_run.finish()\n\n    # save adapters when meaningful\n    if cfg.mode == \"full\" and cfg.get(\"results_dir\"):\n        save_dir = Path(cfg.results_dir) / cfg.run_id\n        save_lora_adapters(model, save_dir)\n\n    return best_val_acc, test_acc\n\n\n# -----------------------------------------------------------------------------\n# Optuna hyper-parameter search (outer run only logs best to wandb)\n# -----------------------------------------------------------------------------\n\ndef _optuna_search(cfg: DictConfig) -> Dict[str, Any]:\n    import optuna\n\n    def objective(trial: optuna.Trial):\n        trial_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n        # --- sample hyper-parameters defined in cfg.optuna.search_space\n        for hp_name, hp_cfg in cfg.optuna.search_space.items():\n            if hp_cfg.type == \"loguniform\":\n                sampled = trial.suggest_float(hp_name, hp_cfg.low, hp_cfg.high, log=True)\n            elif hp_cfg.type == \"uniform\":\n                sampled = trial.suggest_float(hp_name, hp_cfg.low, hp_cfg.high, log=False)\n            else:\n                raise ValueError(f\"Unsupported hp type {hp_cfg.type}\")\n\n            if hp_name == \"beta\":\n                trial_cfg.training.layer_lr_scaling.beta = sampled\n            elif hp_name == \"learning_rate\":\n                trial_cfg.training.optimizer.learning_rate = sampled\n            elif hp_name == \"weight_decay\":\n                trial_cfg.training.optimizer.weight_decay = sampled\n            else:\n                OmegaConf.update(trial_cfg, hp_name, sampled)\n\n        trial_cfg.wandb.mode = \"disabled\"  # disable wandb inside trials\n        best_val, _ = _train_once(trial_cfg)\n        return best_val\n\n    study = optuna.create_study(direction=cfg.optuna.direction)\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n\n    print(\"[Optuna] Best value:\", study.best_value)\n    print(\"[Optuna] Best params:\", study.best_trial.params)\n    return study.best_trial.params\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Merge root config with run-specific YAML\n    run_cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(run_cfg_path)\n    cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n\n    _apply_mode_overrides(cfg)\n    _set_seed(42)\n\n    # Hyper-parameter optimisation (optional)\n    if cfg.optuna.n_trials and cfg.optuna.n_trials > 0:\n        best_params = _optuna_search(cfg)\n        with open_dict(cfg):\n            for k, v in best_params.items():\n                if k == \"beta\":\n                    cfg.training.layer_lr_scaling.beta = v\n                elif k == \"learning_rate\":\n                    cfg.training.optimizer.learning_rate = v\n                elif k == \"weight_decay\":\n                    cfg.training.optimizer.weight_decay = v\n\n    # Final training with (possibly) tuned params\n    _train_once(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "evaluate_py": "import argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy.stats import binomtest\nfrom sklearn.metrics import confusion_matrix\n\n# -----------------------------------------------------------------------------\n# IO helpers\n# -----------------------------------------------------------------------------\n\ndef _json_dump(obj: Dict, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n    print(path)\n\n\n# -----------------------------------------------------------------------------\n# Per-run visualisations\n# -----------------------------------------------------------------------------\n\ndef _export_learning_curve(history_df: pd.DataFrame, run_id: str, out_dir: Path):\n    if \"train_loss\" not in history_df.columns:\n        return\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=np.arange(len(history_df)), y=history_df[\"train_loss\"], label=\"train_loss\")\n    if \"val_accuracy\" in history_df.columns:\n        sns.lineplot(x=np.arange(len(history_df)), y=history_df[\"val_accuracy\"], label=\"val_accuracy\")\n    plt.xlabel(\"Update step\")\n    plt.ylabel(\"Metric value\")\n    plt.title(f\"Learning curve – {run_id}\")\n    plt.legend()\n    plt.tight_layout()\n    fig_path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(fig_path)\n    plt.close()\n    print(fig_path)\n\n\ndef _export_confusion(preds: List[str], golds: List[str], run_id: str, out_dir: Path):\n    uniq = list({*preds, *golds})\n    if len(uniq) == 0 or len(uniq) > 50:\n        return\n    cm = confusion_matrix(golds, preds, labels=uniq)\n    plt.figure(figsize=(max(6, len(uniq) * 0.4), max(4, len(uniq) * 0.4)))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=uniq, yticklabels=uniq)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Gold\")\n    plt.title(f\"Confusion – {run_id}\")\n    plt.tight_layout()\n    fig_path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(fig_path)\n    plt.close()\n    print(fig_path)\n\n\ndef export_run_artifacts(run: \"wandb.apis.public.Run\", out_root: Path):\n    out_root.mkdir(parents=True, exist_ok=True)\n    history_df = run.history()\n    summary = run.summary._json_dict\n    config = dict(run.config)\n\n    _json_dump({\"config\": config, \"summary\": summary, \"history\": history_df.to_dict(\"list\")}, out_root / \"metrics.json\")\n\n    _export_learning_curve(history_df, run.id, out_root)\n\n    if \"_preds\" in summary and \"_golds\" in summary:\n        _export_confusion(summary[\"_preds\"], summary[\"_golds\"], run.id, out_root)\n\n\n# -----------------------------------------------------------------------------\n# Aggregated analysis\n# -----------------------------------------------------------------------------\n\ndef _primary_metric(metrics: Dict[str, Dict[str, float]]) -> str:\n    \"\"\"Pick a sensible primary metric to report.\"\"\"\n    for candidate in [\"test_accuracy\", \"accuracy\", \"val_accuracy\"]:\n        if candidate in metrics:\n            return candidate\n    return list(metrics.keys())[0]\n\n\ndef _mcnemar(p1: List[str], g: List[str], p2: List[str]) -> float:\n    \"\"\"McNemar test p-value (two-sided, exact binomial).\"\"\"\n    assert len(p1) == len(p2) == len(g)\n    n01 = n10 = 0\n    for a, gold, b in zip(p1, g, p2):\n        correct1 = a == gold\n        correct2 = b == gold\n        if correct1 and not correct2:\n            n10 += 1\n        elif correct2 and not correct1:\n            n01 += 1\n    if n01 + n10 == 0:\n        return 1.0\n    bigger = max(n01, n10)\n    return binomtest(bigger, n01 + n10, p=0.5).pvalue\n\n\ndef aggregate(runs: List[\"wandb.apis.public.Run\"], results_dir: Path):\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------- collect numeric metrics ----------------\n    metrics: Dict[str, Dict[str, float]] = {}\n    for r in runs:\n        for k, v in r.summary._json_dict.items():\n            if isinstance(v, (int, float)):\n                metrics.setdefault(k, {})[r.id] = float(v)\n\n    primary = _primary_metric(metrics)\n\n    def _best(label: str):\n        rid, val = None, None\n        for run_id, metric_val in metrics.get(primary, {}).items():\n            if label in run_id:\n                if val is None or metric_val > val:\n                    rid, val = run_id, metric_val\n        return rid, val\n\n    best_prop_id, best_prop_val = _best(\"proposed\")\n    best_base_id, best_base_val = _best(\"comparative\")\n    if best_base_val is None:\n        best_base_id, best_base_val = _best(\"baseline\")\n\n    gap = None\n    if best_prop_val is not None and best_base_val is not None:\n        greater_is_better = not any(word in primary.lower() for word in [\"loss\", \"error\", \"perplexity\"])\n        raw_gap = best_prop_val - best_base_val\n        if not greater_is_better:\n            raw_gap *= -1\n        gap = raw_gap / max(1e-8, abs(best_base_val)) * 100\n\n    # McNemar significance for predictions if available\n    p_value = None\n    if best_prop_id and best_base_id:\n        run_dict = {r.id: r for r in runs}\n        s_prop = run_dict[best_prop_id].summary._json_dict\n        s_base = run_dict[best_base_id].summary._json_dict\n        if all(k in s_prop for k in (\"_preds\", \"_golds\")) and \"_preds\" in s_base:\n            p_value = _mcnemar(s_prop[\"_preds\"], s_prop[\"_golds\"], s_base[\"_preds\"])\n\n    aggregated = {\n        \"primary_metric\": primary,\n        \"metrics\": metrics,\n        \"best_proposed\": {\"run_id\": best_prop_id, \"value\": best_prop_val},\n        \"best_baseline\": {\"run_id\": best_base_id, \"value\": best_base_val},\n        \"gap\": gap,\n        \"mcnemar_p\": p_value,\n    }\n    _json_dump(aggregated, comp_dir / \"aggregated_metrics.json\")\n\n    # ---------------- comparison bar chart ----------------\n    if primary in metrics:\n        ids = list(metrics[primary].keys())\n        vals = [metrics[primary][i] for i in ids]\n        plt.figure(figsize=(max(6, len(ids) * 0.5), 4))\n        sns.barplot(x=ids, y=vals, palette=\"viridis\")\n        for i, v in enumerate(vals):\n            plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n        plt.ylabel(primary)\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fig_path = comp_dir / f\"comparison_{primary}_bar_chart.pdf\"\n        plt.savefig(fig_path)\n        plt.close()\n        print(fig_path)\n\n    # ---------------- boxplot of val_accuracy per run ----------------\n    records = []\n    for r in runs:\n        hist = r.history(keys=[\"val_accuracy\"])  # only fetch required column\n        if \"val_accuracy\" in hist.columns and not hist[\"val_accuracy\"].dropna().empty:\n            for v in hist[\"val_accuracy\"].dropna().tolist():\n                records.append({\"run_id\": r.id, \"val_accuracy\": v})\n\n    if records:\n        df_box = pd.DataFrame(records)\n        plt.figure(figsize=(max(6, len(runs) * 0.6), 4))\n        sns.boxplot(x=\"run_id\", y=\"val_accuracy\", data=df_box, palette=\"pastel\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        fig_path = comp_dir / \"comparison_val_accuracy_boxplot.pdf\"\n        plt.savefig(fig_path)\n        plt.close()\n        print(fig_path)\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list string of run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n\n    # load global wandb settings\n    import yaml as _yaml\n\n    cfg_file = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    with open(cfg_file, \"r\") as f:\n        root_cfg = _yaml.safe_load(f)\n    entity = root_cfg[\"wandb\"][\"entity\"]\n    project = root_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n    runs = []\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        export_run_artifacts(run, results_dir / rid)\n        runs.append(run)\n\n    aggregate(runs, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "preprocess_py": "\"\"\"GSM8K preprocessing pipeline – strict label-leak prevention.\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport datasets\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\nCACHE_DIR = Path(\".cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef extract_numeric_answer(ans: str) -> str:\n    \"\"\"Return substring after first '####' or whole string if absent.\"\"\"\n    return ans.split(\"####\")[-1].strip()\n\n\n# -----------------------------------------------------------------------------\n# Tokeniser\n# -----------------------------------------------------------------------------\n\ndef get_tokenizer(cfg):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=str(CACHE_DIR), use_fast=True)\n    # ensure PAD token exists and left-pad (for generation compatibility)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"left\"\n    return tokenizer\n\n\n# -----------------------------------------------------------------------------\n# Example builders\n# -----------------------------------------------------------------------------\n\ndef _build_train_example(ex: Dict[str, str], tokenizer, max_len: int) -> Dict[str, Any]:\n    prompt = ex[\"question\"].strip() + \"\\n\\nAnswer:\\n####\"\n    prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n\n    answer_text = extract_numeric_answer(ex[\"answer\"]) + tokenizer.eos_token\n    answer_ids = tokenizer(answer_text, add_special_tokens=False)[\"input_ids\"]\n\n    placeholder_ids = [tokenizer.pad_token_id] * len(answer_ids)  # prevents label leakage\n\n    input_ids = (prompt_ids + placeholder_ids)[-max_len:]\n    attention_mask = [1] * len(input_ids)\n\n    labels = ([-100] * len(prompt_ids) + answer_ids)[-max_len:]\n    if len(labels) < len(input_ids):\n        labels = [-100] * (len(input_ids) - len(labels)) + labels\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ndef _build_eval_example(ex: Dict[str, str], tokenizer, max_len: int) -> Dict[str, Any]:\n    prompt = ex[\"question\"].strip() + \"\\n\\nAnswer:\\n####\"\n    enc = tokenizer(prompt, add_special_tokens=False, truncation=True, max_length=max_len)\n    return {\n        \"input_ids\": enc[\"input_ids\"],\n        \"attention_mask\": enc[\"attention_mask\"],\n        \"labels\": extract_numeric_answer(ex[\"answer\"]),\n    }\n\n\n# -----------------------------------------------------------------------------\n# Collator\n# -----------------------------------------------------------------------------\n\ndef _collate_fn(tokenizer, is_train: bool):\n    def collate(features: List[Dict[str, Any]]):\n        input_ids = [f[\"input_ids\"] for f in features]\n        attention = [f[\"attention_mask\"] for f in features]\n        batch_enc = tokenizer.pad({\"input_ids\": input_ids, \"attention_mask\": attention}, return_tensors=\"pt\")\n\n        if is_train:\n            labels_list = [f[\"labels\"] for f in features]\n            max_len = max(len(l) for l in labels_list)\n            labels_tensor = torch.full((len(labels_list), max_len), -100, dtype=torch.long)\n            for i, seq in enumerate(labels_list):\n                labels_tensor[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n            batch_enc[\"labels\"] = labels_tensor\n        else:\n            batch_enc[\"labels\"] = [f[\"labels\"] for f in features]\n        return batch_enc\n\n    return collate\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg, tokenizer) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    ds = load_dataset(cfg.dataset.name, cfg.dataset.get(\"config\", None), cache_dir=str(CACHE_DIR))\n\n    split = ds[\"train\"].train_test_split(test_size=0.05, seed=42)\n    train_ds, val_ds = split[\"train\"], split[\"test\"]\n    test_ds = ds[\"test\"]\n\n    max_len = cfg.dataset.max_seq_length\n\n    train_ds = train_ds.map(lambda x: _build_train_example(x, tokenizer, max_len), remove_columns=train_ds.column_names)\n    val_ds = val_ds.map(lambda x: _build_eval_example(x, tokenizer, max_len), remove_columns=val_ds.column_names)\n    test_ds = test_ds.map(lambda x: _build_eval_example(x, tokenizer, max_len), remove_columns=test_ds.column_names)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        collate_fn=_collate_fn(tokenizer, is_train=True),\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        collate_fn=_collate_fn(tokenizer, is_train=False),\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=1,\n        shuffle=False,\n        collate_fn=_collate_fn(tokenizer, is_train=False),\n    )\n\n    return train_loader, val_loader, test_loader\n",
            "model_py": "\"\"\"Model utilities – loading, optimisation, evaluation, adapter saving.\"\"\"\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    get_cosine_schedule_with_warmup,\n)\n\ntry:\n    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nexcept ImportError:\n    # Graceful degradation: training without LoRA if peft unavailable\n    LoraConfig = None\n\nCACHE_DIR = Path(\".cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\n# -----------------------------------------------------------------------------\n# Model loading\n# -----------------------------------------------------------------------------\n\ndef load_lm_and_prepare_for_training(cfg, device: torch.device):\n    \"\"\"Load a language model according to cfg, optionally with 4-bit quantisation & LoRA.\n    Falls back to fp16/bf16 on CPU when 4-bit is requested but CUDA unavailable.\n    \"\"\"\n    use_4bit = cfg.model.precision == \"4bit\" and torch.cuda.is_available()\n\n    if cfg.model.precision == \"4bit\" and not torch.cuda.is_available():\n        print(\"[WARN] 4-bit quantisation requested but CUDA not available – falling back to fp16.\")\n\n    quant_cfg = None\n    if use_4bit:\n        quant_cfg = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=str(CACHE_DIR),\n        quantization_config=quant_cfg,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n    )\n\n    # ---------------- LoRA adapters ----------------\n    if cfg.model.adapter.type == \"lora\":\n        if LoraConfig is None:\n            raise ImportError(\"peft package required for LoRA adapters but not found.\")\n        lora_cfg = LoraConfig(\n            r=cfg.model.adapter.r,\n            lora_alpha=cfg.model.adapter.alpha,\n            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n            lora_dropout=cfg.model.adapter.dropout,\n            task_type=\"CAUSAL_LM\",\n        )\n        if use_4bit:\n            model = prepare_model_for_kbit_training(model)\n        model = get_peft_model(model, lora_cfg)\n        model.print_trainable_parameters()\n\n    model.to(device)\n    return model\n\n# -----------------------------------------------------------------------------\n# Optimiser & scheduler with ALLRS\n# -----------------------------------------------------------------------------\n\ndef _layer_id_from_name(name: str) -> int:\n    parts = name.split(\".\")\n    for p in parts:\n        if p.isdigit():\n            return int(p)\n    return -1\n\n\ndef _compute_layer_lr_scale(name: str, num_layers: int, beta: float) -> float:\n    if any(tok in name for tok in [\".layers.\", \"transformer.h.\"]):\n        idx = _layer_id_from_name(name)\n        if idx >= 0:\n            return beta ** (num_layers - 1 - idx)\n    return 1.0\n\n\ndef build_optimizer_with_layerwise_scaling(cfg, model, train_loader_len: int):\n    base_lr = cfg.training.optimizer.learning_rate\n    wd = cfg.training.optimizer.weight_decay\n    beta = cfg.training.layer_lr_scaling.beta if cfg.training.layer_lr_scaling.enabled else 1.0\n\n    # attempt to infer number of layers from model config\n    num_layers = (\n        getattr(model.config, \"num_hidden_layers\", None)\n        or getattr(model.config, \"n_layer\", None)\n        or getattr(model.config, \"num_layers\", 0)\n    )\n\n    param_groups = {}\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        scale = _compute_layer_lr_scale(name, num_layers, beta)\n        lr = base_lr * scale\n        key = f\"lr{lr}_wd{wd}\"\n        if key not in param_groups:\n            param_groups[key] = {\"params\": [], \"lr\": lr, \"weight_decay\": wd}\n        param_groups[key][\"params\"].append(param)\n\n    optimizer = torch.optim.AdamW(list(param_groups.values()), betas=tuple(cfg.training.optimizer.betas), eps=cfg.training.optimizer.eps)\n\n    steps_per_epoch = train_loader_len // cfg.training.gradient_accumulation_steps\n    if cfg.training.max_train_batches:\n        steps_per_epoch = min(steps_per_epoch, cfg.training.max_train_batches // cfg.training.gradient_accumulation_steps)\n    total_steps = max(1, steps_per_epoch * cfg.training.epochs)\n\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.training.scheduler.warmup_steps,\n        num_training_steps=total_steps,\n    )\n    return optimizer, scheduler, total_steps\n\n# -----------------------------------------------------------------------------\n# Evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef evaluate_accuracy(model, tokenizer, loader, device, cfg) -> Tuple[float, list, list]:\n    model.eval()\n    preds, golds = [], []\n    correct = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_new_tokens=64,\n                do_sample=False,\n            )\n            input_lens = (input_ids != tokenizer.pad_token_id).sum(-1)\n            for i in range(outputs.size(0)):\n                gen_ids = outputs[i][input_lens[i] :]\n                pred = tokenizer.decode(gen_ids, skip_special_tokens=True)\n                gold = batch[\"labels\"][i]\n                preds.append(pred.strip())\n                golds.append(gold.strip())\n                if pred.strip().split(\"####\")[-1].strip() == gold.strip():\n                    correct += 1\n            if cfg.mode == \"trial\" and batch_idx >= 20:\n                break\n    total = len(preds) if preds else 1\n    acc = correct / total\n    model.train()\n    return acc, preds, golds\n\n# -----------------------------------------------------------------------------\n# Adapter saver\n# -----------------------------------------------------------------------------\n\ndef save_lora_adapters(model, out_dir: Path):\n    try:\n        out_dir.mkdir(parents=True, exist_ok=True)\n        model.save_pretrained(out_dir)\n    except Exception as e:\n        print(f\"[WARN] Saving adapters failed: {e}\")\n",
            "main_py": "\"\"\"Hydra orchestrator – spawns src.train in a subprocess.\"\"\"\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf, open_dict\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_cfg_path = CONFIG_DIR / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(run_cfg_path)\n\n    # Build composed cfg to inspect/override here (does not propagate to train)\n    full_cfg = OmegaConf.merge(cfg, OmegaConf.load(run_cfg_path))\n    with open_dict(full_cfg):\n        if full_cfg.mode == \"trial\":\n            full_cfg.wandb.mode = \"disabled\"\n            full_cfg.optuna.n_trials = 0\n            full_cfg.training.epochs = 1\n            full_cfg.training.max_train_batches = 2\n        elif full_cfg.mode == \"full\":\n            full_cfg.wandb.mode = \"online\"\n\n    # Save composed cfg for debugging\n    composed_path = Path.cwd() / \"composed_cfg.yaml\"\n    OmegaConf.save(full_cfg, composed_path)\n\n    # Spawn training subprocess (uses hydra again inside)\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"Launching:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n    composed_path.unlink(missing_ok=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "pyproject_toml": "[project]\nname = \"allrs-gsm8k-experiments\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.1.0\"\ntransformers = \">=4.37.0\"\ndatasets = \">=2.15.0\"\npeft = \">=0.6.2\"\nbitsandbytes = \">=0.41.1\"\nhydra-core = \">=1.3.2\"\nwandb = \">=0.16.0\"\noptuna = \">=3.5.0\"\nmatplotlib = \">=3.8.0\"\nseaborn = \">=0.13.0\"\ntqdm = \">=4.66.1\"\nscikit-learn = \">=1.3.2\"\nscipy = \">=1.11.0\"\n",
            "config_yaml": "defaults:\n  - _self_\n\nrun: \"???\"            # supplied via CLI – points to YAML in config/runs\nmode: full             # full | trial (overridden via CLI)\nresults_dir: ./results\n\nwandb:\n  entity: gengaru617-personal\n  project: 2025-11-17\n  mode: online         # switched to disabled in trial\n\n# stub sections – will be overwritten by run-specific YAML ---------------------\nmodel: {}\ndataset: {}\ntraining:\n  max_train_batches: null\noptuna:\n  n_trials: 0\n"
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k",
            "method_name": "proposed",
            "model_name": "Qwen3-0.6B (4-bit LoRA-tuned)",
            "dataset_name": "gsm8k",
            "run_config": "run_id: proposed-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k\nmethod: proposed\nmodel:\n  name: Qwen/Qwen3-0.6B\n  precision: 4bit\n  adapter:\n    type: lora\n    r: 64\n    alpha: 16\n    dropout: 0.05\ndataset:\n  name: gsm8k\n  config: main\n  max_seq_length: 512\n  text_column: question\n  label_column: answer\n  pack_sequences: true\ntraining:\n  epochs: 3\n  batch_size: 1                 # real batch per device\n  gradient_accumulation_steps: 8 # effective batch = 8\n  optimizer:\n    type: adamw\n    learning_rate: 2e-5         # global LR before scaling\n    weight_decay: 0.1\n    betas: [0.9, 0.999]\n    eps: 1e-8\n  scheduler:\n    type: cosine\n    warmup_steps: 500\n  layer_lr_scaling:\n    enabled: true\n    beta: 0.9                   # lr_l(t)=lr_global(t)*beta^(L-l)\n    strategy: exponential_depth\nevaluation:\n  decoding_temperature: 0.0\n  strategy: greedy\nhardware:\n  device: cpu\n  max_memory_mb: 500\noptuna:\n  n_trials: 30\n  direction: maximize  # maximise GSM8K accuracy\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-5\n    weight_decay:\n      type: uniform\n      low: 0.0\n      high: 0.1\n    beta:\n      type: uniform\n      low: 0.7\n      high: 0.95\n"
          },
          {
            "run_id": "comparative-1-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k",
            "method_name": "comparative-1",
            "model_name": "Qwen3-0.6B (4-bit LoRA-tuned)",
            "dataset_name": "gsm8k",
            "run_config": "run_id: comparative-1-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k\nmethod: baseline\nmodel:\n  name: Qwen/Qwen3-0.6B\n  precision: 4bit\n  adapter:\n    type: lora\n    r: 64\n    alpha: 16\n    dropout: 0.05\ndataset:\n  name: gsm8k\n  config: main\n  max_seq_length: 512\n  text_column: question\n  label_column: answer\n  pack_sequences: true\ntraining:\n  epochs: 3\n  batch_size: 1\n  gradient_accumulation_steps: 8\n  optimizer:\n    type: adamw\n    learning_rate: 2e-5\n    weight_decay: 0.1\n    betas: [0.9, 0.999]\n    eps: 1e-8\n  scheduler:\n    type: cosine\n    warmup_steps: 500\n  layer_lr_scaling:\n    enabled: false\n    beta: 1.0\n    strategy: none\nevaluation:\n  decoding_temperature: 0.0\n  strategy: greedy\nhardware:\n  device: cpu\n  max_memory_mb: 500\noptuna:\n  n_trials: 20\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-5\n    weight_decay:\n      type: uniform\n      low: 0.0\n      high: 0.1\n"
          }
        ]
      }
    ]
  }
}