{
  "research_topic": "Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems",
  "queries": [
    "Qwen3-0.6B fine-tuning",
    "learning rate optimization",
    "GSM8K fine-tuning",
    "adaptive learning rate",
    "elementary math LLM"
  ],
  "research_study_list": [
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
      "meta_data": {
        "arxiv_id": "2305.14314v1",
        "authors": [
          "Tim Dettmers",
          "Artidoro Pagnoni",
          "Ari Holtzman",
          "Luke Zettlemoyer"
        ],
        "published_date": "2023-05-23T17:50:33Z",
        "pdf_url": "https://arxiv.org/pdf/2305.14314v1.pdf"
      }
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
      "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR applied to every $τ$ steps that minimizes the resulted validation loss. We solve this black-box optimization on the fly by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for $τ$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only $τ'\\llτ$ steps and train an exponential model to predict the validation loss after $τ$ steps. This mutual-training process between BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of $1.31\\times$ over state-of-the-art heavily-tuned LR schedules.",
      "meta_data": {
        "arxiv_id": "2105.10762v1",
        "authors": [
          "Yuchen Jin",
          "Tianyi Zhou",
          "Liangyu Zhao",
          "Yibo Zhu",
          "Chuanxiong Guo",
          "Marco Canini",
          "Arvind Krishnamurthy"
        ],
        "published_date": "2021-05-22T16:41:10Z",
        "pdf_url": "https://arxiv.org/pdf/2105.10762v1.pdf"
      }
    },
    {
      "title": "Reverse engineering learned optimizers reveals known and novel mechanisms",
      "abstract": "Learned optimizers are algorithms that can themselves be trained to solve optimization problems. In contrast to baseline optimizers (such as momentum or Adam) that use simple update rules derived from theoretical principles, learned optimizers use flexible, high-dimensional, nonlinear parameterizations. Although this can lead to better performance in certain settings, their inner workings remain a mystery. How is a learned optimizer able to outperform a well tuned baseline? Has it learned a sophisticated combination of existing optimization techniques, or is it implementing completely new behavior? In this work, we address these questions by careful analysis and visualization of learned optimizers. We study learned optimizers trained from scratch on three disparate tasks, and discover that they have learned interpretable mechanisms, including: momentum, gradient clipping, learning rate schedules, and a new form of learning rate adaptation. Moreover, we show how the dynamics of learned optimizers enables these behaviors. Our results help elucidate the previously murky understanding of how learned optimizers work, and establish tools for interpreting future learned optimizers.",
      "meta_data": {
        "arxiv_id": "2011.02159v2",
        "authors": [
          "Niru Maheswaranathan",
          "David Sussillo",
          "Luke Metz",
          "Ruoxi Sun",
          "Jascha Sohl-Dickstein"
        ],
        "published_date": "2020-11-04T07:12:43Z",
        "pdf_url": "https://arxiv.org/pdf/2011.02159v2.pdf"
      }
    },
    {
      "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
      "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
      "meta_data": {
        "arxiv_id": "2309.12284v4",
        "authors": [
          "Longhui Yu",
          "Weisen Jiang",
          "Han Shi",
          "Jincheng Yu",
          "Zhengying Liu",
          "Yu Zhang",
          "James T. Kwok",
          "Zhenguo Li",
          "Adrian Weller",
          "Weiyang Liu"
        ],
        "published_date": "2023-09-21T17:45:42Z",
        "pdf_url": "https://arxiv.org/pdf/2309.12284v4.pdf"
      }
    },
    {
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
      "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",
      "meta_data": {
        "arxiv_id": "2402.10176v2",
        "authors": [
          "Shubham Toshniwal",
          "Ivan Moshkov",
          "Sean Narenthiran",
          "Daria Gitman",
          "Fei Jia",
          "Igor Gitman"
        ],
        "published_date": "2024-02-15T18:26:11Z",
        "pdf_url": "https://arxiv.org/pdf/2402.10176v2.pdf"
      }
    },
    {
      "title": "MoMo: Momentum Models for Adaptive Learning Rates",
      "abstract": "Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic gradient descent with momentum). MoMo uses momentum estimates of the losses and gradients sampled at each iteration to build a model of the loss function. Our model makes use of any known lower bound of the loss function by using truncation, e.g. most losses are lower-bounded by zero. The model is then approximately minimized at each iteration to compute the next step. We show how MoMo can be used in combination with any momentum-based method, and showcase this by developing MoMo-Adam, which is Adam with our new model-based adaptive learning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for convex problems with interpolation, needing knowledge of no problem-specific quantities other than the optimal value. Additionally, for losses with unknown lower bounds, we develop on-the-fly estimates of a lower bound, that are incorporated in our model. We show that MoMo and MoMo-Adam improve over SGD-M and Adam in terms of robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR, and Imagenet, for recommender systems on Criteo, for a transformer model on the translation task IWSLT14, and for a diffusion model.",
      "meta_data": {
        "arxiv_id": "2305.07583v3",
        "authors": [
          "Fabian Schaipp",
          "Ruben Ohana",
          "Michael Eickenberg",
          "Aaron Defazio",
          "Robert M. Gower"
        ],
        "published_date": "2023-05-12T16:25:57Z",
        "pdf_url": "https://arxiv.org/pdf/2305.07583v3.pdf"
      }
    },
    {
      "title": "Llemma: An Open Language Model for Mathematics",
      "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
      "meta_data": {
        "arxiv_id": "2310.10631v3",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "published_date": "2023-10-16T17:54:07Z",
        "pdf_url": "https://arxiv.org/pdf/2310.10631v3.pdf"
      }
    },
    {
      "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
      "abstract": "Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with several families of models showing evidence of systematic overfitting across almost all model sizes. Further analysis suggests a positive relationship (Spearman's r^2 = 0.36) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that some models may have partially memorized GSM8k. Nevertheless, many models, especially those on the frontier, show minimal signs of overfitting, and all models broadly demonstrate generalization to novel math problems guaranteed to not be in their training data.",
      "meta_data": {
        "arxiv_id": "2405.00332v4",
        "authors": [
          "Hugh Zhang",
          "Jeff Da",
          "Dean Lee",
          "Vaughn Robinson",
          "Catherine Wu",
          "Will Song",
          "Tiffany Zhao",
          "Pranav Raja",
          "Charlotte Zhuang",
          "Dylan Slack",
          "Qin Lyu",
          "Sean Hendryx",
          "Russell Kaplan",
          "Michele Lunati",
          "Summer Yue"
        ],
        "published_date": "2024-05-01T05:52:05Z",
        "pdf_url": "https://arxiv.org/pdf/2405.00332v4.pdf"
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "Current fine-tuning of large LLMs such as Qwen3-0.6B on GSM8K uses a single global learning-rate schedule. This often leads to (1) excessive updates on the upper transformer blocks, causing catastrophic forgetting, while (2) lower blocks remain under-adapted, limiting mathematical reasoning depth. The key limitation is the lack of layer-wise differentiation in learning-rate magnitude, which could be solved with a very small change to the optimisation procedure.",
        "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
        "experimental_setup": "1. Model: Qwen3-0.6B (HF Transformers)\n2. Dataset: GSM8K train / test split (as in Cobbe et al.)\n3. Baseline: Standard fine-tuning with global learning rate 2e-5, 500 warm-up steps, cosine decay.\n4. Proposed: Same schedule + ALLRS with β=0.9.\n5. Optimiser: AdamW, weight-decay 0.1.\n6. Training budget: 3 epochs, batch size 8 (fits on a single A100 with gradient accumulation).\n7. Evaluation: Generate answers with greedy decoding (temperature=0) and compare against ground-truth numeric answers.\n8. Comparison: Report accuracy on GSM8K test set; measure training stability (loss curve).",
        "primary_metric": "accuracy",
        "experimental_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# build parameter groups with layer-wise lr scale\nbeta = 0.9\nlayer_to_lr_scale = {}\nfor name, _ in model.named_parameters():\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])   # transformer.h.{idx}.*\n        layer_to_lr_scale.setdefault(layer_id, beta**(model.config.num_hidden_layers-1-layer_id))\n\nparam_groups = []\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])\n        scale = layer_to_lr_scale[layer_id]\n    else:\n        scale = 1.0  # embeddings, ln_f, lm_head\n    param_groups.append({\"params\": [param], \"lr_scale\": scale})\n\nglobal_lr = 2e-5\noptimizer = AdamW(param_groups, lr=global_lr, weight_decay=0.1)\n\n# attach scheduler\nnum_training_steps = 6000\nnum_warmup_steps = 500\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\n# in the training loop\nfor step, batch in enumerate(train_loader):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    # apply individual lr scales\n    for g in optimizer.param_groups:\n        g[\"lr\"] = scheduler.get_last_lr()[0] * g[\"lr_scale\"]",
        "expected_result": "Baseline fine-tuning (global LR): ~55% GSM8K accuracy, occasional training instability after epoch 2.\nALLRS (β=0.9): expected 58-60% accuracy (+3-5 points) with smoother validation loss curve and no extra compute cost. Per-token perplexity should drop ~3% relative to baseline by the end of training.",
        "expected_conclusion": "A single multiplicative depth factor (β) is enough to rectify the imbalance of updates across transformer layers. This minimal change is trivial to implement, adds negligible overhead, and empirically improves GSM8K accuracy by several points, demonstrating that careful layer-wise learning-rate allocation is a practical lever for boosting mathematical reasoning in LLM fine-tuning."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces an exponential layer-wise decay factor β during fine-tuning of Qwen3-0.6B. However, layer-wise learning-rate decay (LLRD) or scaling has already been explored in several contexts: (1) BERT fine-tuning for NLP (e.g., Howard & Ruder 2018 ‘ULMFiT’, Sun et al. 2019 ‘Fine-tuning BERT…’), (2) ViT training where an almost identical formula lr_ℓ = lr·β^(L-ℓ) is standard, and (3) LARS/LAMB optimizers that adaptively modify per-layer update magnitude. The only novel aspect is applying this well-known trick specifically to GSM8K mathematical-reasoning fine-tuning of a small Qwen model and empirically framing it as a remedy for catastrophic forgetting in upper blocks. No new algorithmic insight beyond prior LLRD literature is presented; the work is an application/confirmation in a new data-model regime rather than a conceptual advance.",
        "novelty_score": 4,
        "significance_reason": "Despite limited methodological novelty, the hypothesis could have practical impact: GSM8K is a widely-used benchmark and Qwen3-0.6B is within the resource envelope of many practitioners. If a single hyper-parameter can add 3–5 percentage-points of accuracy with zero extra compute, the technique is valuable for the community fine-tuning small LLMs under tight budgets. Academically, it provides evidence that layer-wise LR allocation helps reasoning tasks—not just generic GLUE or vision. Societally, improved math-reasoning at lower compute cost broadens access to educational or tutoring systems. However, the incremental nature of the gain and the narrow scope (one model, one dataset) temper its overall significance.",
        "significance_score": 6
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "Current fine-tuning of large LLMs such as Qwen3-0.6B on GSM8K uses a single global learning-rate schedule. This often leads to (1) excessive updates on the upper transformer blocks, causing catastrophic forgetting, while (2) lower blocks remain under-adapted, limiting mathematical reasoning depth. The key limitation is the lack of layer-wise differentiation in learning-rate magnitude, which could be solved with a very small change to the optimisation procedure.",
      "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
      "experimental_setup": "1. Model: Qwen3-0.6B (HF Transformers)\n2. Dataset: GSM8K train / test split (as in Cobbe et al.)\n3. Baseline: Standard fine-tuning with global learning rate 2e-5, 500 warm-up steps, cosine decay.\n4. Proposed: Same schedule + ALLRS with β=0.9.\n5. Optimiser: AdamW, weight-decay 0.1.\n6. Training budget: 3 epochs, batch size 8 (fits on a single A100 with gradient accumulation).\n7. Evaluation: Generate answers with greedy decoding (temperature=0) and compare against ground-truth numeric answers.\n8. Comparison: Report accuracy on GSM8K test set; measure training stability (loss curve).",
      "primary_metric": "accuracy",
      "experimental_code": "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# build parameter groups with layer-wise lr scale\nbeta = 0.9\nlayer_to_lr_scale = {}\nfor name, _ in model.named_parameters():\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])   # transformer.h.{idx}.*\n        layer_to_lr_scale.setdefault(layer_id, beta**(model.config.num_hidden_layers-1-layer_id))\n\nparam_groups = []\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    if name.startswith(\"transformer.h.\"):\n        layer_id = int(name.split(\".\")[2])\n        scale = layer_to_lr_scale[layer_id]\n    else:\n        scale = 1.0  # embeddings, ln_f, lm_head\n    param_groups.append({\"params\": [param], \"lr_scale\": scale})\n\nglobal_lr = 2e-5\noptimizer = AdamW(param_groups, lr=global_lr, weight_decay=0.1)\n\n# attach scheduler\nnum_training_steps = 6000\nnum_warmup_steps = 500\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n\n# in the training loop\nfor step, batch in enumerate(train_loader):\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    # apply individual lr scales\n    for g in optimizer.param_groups:\n        g[\"lr\"] = scheduler.get_last_lr()[0] * g[\"lr_scale\"]",
      "expected_result": "Baseline fine-tuning (global LR): ~55% GSM8K accuracy, occasional training instability after epoch 2.\nALLRS (β=0.9): expected 58-60% accuracy (+3-5 points) with smoother validation loss curve and no extra compute cost. Per-token perplexity should drop ~3% relative to baseline by the end of training.",
      "expected_conclusion": "A single multiplicative depth factor (β) is enough to rectify the imbalance of updates across transformer layers. This minimal change is trivial to implement, adds negligible overhead, and empirically improves GSM8K accuracy by several points, demonstrating that careful layer-wise learning-rate allocation is a practical lever for boosting mathematical reasoning in LLM fine-tuning."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nModification: keep the standard warm-up + cosine decay schedule, but multiply the learning rate of each transformer block ℓ by an exponential depth factor β^(L-ℓ), where L is the index of the last block and 0<β≤1 (e.g. β=0.9). Thus\n    lr_ℓ(t) = lr_global(t) * β^(L-ℓ)\nOnly one additional hyper-parameter (β) is introduced.\nMotivation: Upper layers capture task-specific features and already change quickly; lower layers store general linguistic/mathematical priors and should change more cautiously. Exponentially damped LR prevents over-fitting at the top while still allowing gradual adaptation of the bottom, improving reasoning consistency with minimal implementation effort.",
        "experimental_design": {
          "experiment_summary": "Goal: show that Adaptive Layer-wise Learning-Rate Scaling (ALLRS) yields higher answer-accuracy on elementary maths word-problems than the usual single global learning-rate.\nTask: given a GSM8K problem in natural language, generate a step-by-step chain-of-thought that ends with the correct numeric answer. The prediction is considered correct only if the final number exactly matches the gold answer.\nWorkflow:\n1. Load Qwen3-0.6B in 4-bit quantised format to stay inside the 500 MB RAM cap.\n2. Add LoRA adapters to all self-attention and MLP blocks so only ~6 M parameters are trainable.\n3. Prepare the GSM8K train / test splits; tokenise with the Qwen3 tokenizer; pack to 512-token sequences.\n4. Train for 1 epoch with gradient-accumulation to simulate batch-size 8.\n   • Baseline run: AdamW + warm-up 10 % steps + cosine decay, single global learning-rate.\n   • Proposed run: identical schedule but per-block rate lr_l(t)=lr_global(t)·β^(L-l) with β searched in [0.7,0.95].  Implementation: keep one AdamW optimiser, but store a constant lr_scale in every param-group and multiply the scheduler-produced LR by that factor each step.\n5. Decode each test question greedily (temperature 0) and strip everything after the first ‘####’ token to obtain the predicted answer string.\n6. Compute metrics; plot training-loss curves for stability comparison.\n7. Compare final accuracy and learning-curve smoothness; expect +3-5 pp accuracy for ALLRS without extra compute.",
          "evaluation_metrics": [
            {
              "name": "accuracy",
              "description": "Correctness criteria: a prediction is correct if the substring after the first token sequence \"####\" in the model output, stripped of whitespace, exactly equals the gold numeric answer string; for fractional answers the comparison is performed on the unreduced fraction so 3/6≠1/2.\nCalculation: accuracy = (#correct predictions)/(#total test questions).\nTask appropriateness: GSM8K is a single-answer task where each example has one canonical numeric solution; therefore simple accuracy directly measures problem-solving success.\nVisualisations: bar chart comparing baseline vs ALLRS accuracies; 95 % binomial confidence intervals overlaid."
            },
            {
              "name": "training_loss",
              "description": "Correctness criteria: not applicable; this is a descriptive curve.\nCalculation: at every optimisation step record the cross-entropy loss returned by the model; after training, smooth with an exponential moving average (α=0.98) and plot step vs loss.\nTask appropriateness: reveals optimisation stability; a smoother, monotonically decreasing curve indicates better-behaved updates, which ALLRS claims to deliver.\nVisualisations: line plot of smoothed loss for baseline and ALLRS runs on the same axes."
            }
          ],
          "proposed_method": "Adaptive Layer-wise Learning-Rate Scaling (ALLRS)\nObjective: mitigate catastrophic forgetting in upper layers and under-adaptation in lower layers during fine-tuning by assigning smaller learning-rates to deeper (closer to embeddings) transformer blocks.\nTheory: let the standard scheduler output a global rate lr_g(t).  For a model with last layer index L, block ℓ uses lr_ℓ(t)=lr_g(t)·β^(L-ℓ), with damping factor 0<β≤1.  Exponential decay maintains the ratio of consecutive layers’ step-sizes, leading to gradient norms that vary smoothly with depth.\nImplementation steps:\n1. Traverse model.named_parameters(); group parameters by transformer block index ℓ, assign constant multiplier β^(L-ℓ) to that group (embeddings, final LN and lm_head get scale 1).\n2. Create a single AdamW optimiser where each param-group stores the field \"lr_scale\".\n3. At every scheduler step, set group[\"lr\"] = scheduler_lr * group[\"lr_scale\"].  No other change to training loop.\n4. Only one new hyper-parameter β is introduced, making grid-search inexpensive.\n5. Compatible with LoRA (only adapter weights receive scaled rates) and with any optimiser that exposes per-group learning-rates.",
          "comparative_methods": [
            "Standard Fine-tuning with single global learning-rate schedule"
          ],
          "models_to_use": [
            "Qwen3-0.6B (4-bit LoRA-tuned)"
          ],
          "datasets_to_use": [
            "gsm8k"
          ],
          "hyperparameters_to_search": [
            {
              "name": "beta",
              "range": "0.7-0.95"
            },
            {
              "name": "learning_rate",
              "range": "1e-5-5e-5"
            },
            {
              "name": "weight_decay",
              "range": "0-0.1"
            }
          ]
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k",
            "method_name": "proposed",
            "model_name": "Qwen3-0.6B (4-bit LoRA-tuned)",
            "dataset_name": "gsm8k"
          },
          {
            "run_id": "comparative-1-iter1-Qwen3-0.6B-4-bit-LoRA-tuned-gsm8k",
            "method_name": "comparative-1",
            "model_name": "Qwen3-0.6B (4-bit LoRA-tuned)",
            "dataset_name": "gsm8k"
          }
        ]
      }
    ]
  }
}